{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9hHZE3SY68N8FxNv4rSlR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RRDajay/Machine_Learning_Collection/blob/master/Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mksIQiXvKTzi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a3824dfb-0372-42f0-8518-ccf130790417"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim \n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 1000\n",
        "num_epochs = 25\n",
        "\n",
        "# Load data\n",
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='dataset/', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test_set = torchvision.datasets.CIFAR10(root='dataset/', train=False, transform=transform, download=True)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXryRGE-p0O7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "aad8af56-83ac-4702-b0a8-49fcdaba022e"
      },
      "source": [
        "class Identity(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x\n",
        "\n",
        "# GPU or CPU selection\n",
        "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "\n",
        "# Model Creation\n",
        "\n",
        "model = torchvision.models.vgg16(pretrained=True, progress=True)\n",
        "# Freeze model\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Create new linear layer, remove avgpool layer\n",
        "model.avgpool = Identity()\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(),\n",
        "    nn.BatchNorm1d(num_features=512),\n",
        "    nn.Linear(512, 10)\n",
        "    )\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): Identity()\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ief4k2Nrc0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizer and Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Scheduler\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Tmjt2irDH2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "outputId": "7fd679d7-69e5-4976-f5c0-9dd58d6cbce9"
      },
      "source": [
        "# Model training \n",
        "n_total_steps =len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "    data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "    # Forward\n",
        "    scores = model(data)\n",
        "    loss = criterion(scores, targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # Backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Grad Descent\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "    if (batch_idx) % 25 == 0:\n",
        "      print(f\"Epoch: {epoch+1}/{num_epochs}\\t\\tStep: {batch_idx+1}/{n_total_steps}\\t\\tloss: {sum(losses)/len(losses)}\")\n",
        "  \n",
        "  scheduler.step(sum(losses)/len(losses)) \n",
        "  \n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/25\t\tStep: 1/50\t\tloss: 1.3982295989990234\n",
            "Epoch: 1/25\t\tStep: 26/50\t\tloss: 1.3492299685111413\n",
            "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch: 2/25\t\tStep: 1/50\t\tloss: 1.3563973903656006\n",
            "Epoch: 2/25\t\tStep: 26/50\t\tloss: 1.357703447341919\n",
            "Epoch: 3/25\t\tStep: 1/50\t\tloss: 1.3689690828323364\n",
            "Epoch: 3/25\t\tStep: 26/50\t\tloss: 1.3506311361606305\n",
            "Epoch: 4/25\t\tStep: 1/50\t\tloss: 1.3469504117965698\n",
            "Epoch: 4/25\t\tStep: 26/50\t\tloss: 1.3491741327139049\n",
            "Epoch: 5/25\t\tStep: 1/50\t\tloss: 1.2829779386520386\n",
            "Epoch: 5/25\t\tStep: 26/50\t\tloss: 1.3479869136443505\n",
            "Epoch: 6/25\t\tStep: 1/50\t\tloss: 1.3154040575027466\n",
            "Epoch: 6/25\t\tStep: 26/50\t\tloss: 1.3434474835029016\n",
            "Epoch: 7/25\t\tStep: 1/50\t\tloss: 1.338165283203125\n",
            "Epoch: 7/25\t\tStep: 26/50\t\tloss: 1.352046618094811\n",
            "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch: 8/25\t\tStep: 1/50\t\tloss: 1.334945797920227\n",
            "Epoch: 8/25\t\tStep: 26/50\t\tloss: 1.3386414463703449\n",
            "Epoch: 9/25\t\tStep: 1/50\t\tloss: 1.3962491750717163\n",
            "Epoch: 9/25\t\tStep: 26/50\t\tloss: 1.349084904560676\n",
            "Epoch: 10/25\t\tStep: 1/50\t\tloss: 1.378987431526184\n",
            "Epoch: 10/25\t\tStep: 26/50\t\tloss: 1.346292596596938\n",
            "Epoch: 11/25\t\tStep: 1/50\t\tloss: 1.2844783067703247\n",
            "Epoch: 11/25\t\tStep: 26/50\t\tloss: 1.3419263454583974\n",
            "Epoch: 12/25\t\tStep: 1/50\t\tloss: 1.3375422954559326\n",
            "Epoch: 12/25\t\tStep: 26/50\t\tloss: 1.3447572726469774\n",
            "Epoch: 13/25\t\tStep: 1/50\t\tloss: 1.3674675226211548\n",
            "Epoch: 13/25\t\tStep: 26/50\t\tloss: 1.3519545518434966\n",
            "Epoch: 14/25\t\tStep: 1/50\t\tloss: 1.3642152547836304\n",
            "Epoch: 14/25\t\tStep: 26/50\t\tloss: 1.3549824769680316\n",
            "Epoch: 15/25\t\tStep: 1/50\t\tloss: 1.3636714220046997\n",
            "Epoch: 15/25\t\tStep: 26/50\t\tloss: 1.346114273254688\n",
            "Epoch: 16/25\t\tStep: 1/50\t\tloss: 1.3028520345687866\n",
            "Epoch: 16/25\t\tStep: 26/50\t\tloss: 1.3521907237859874\n",
            "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch: 17/25\t\tStep: 1/50\t\tloss: 1.3502352237701416\n",
            "Epoch: 17/25\t\tStep: 26/50\t\tloss: 1.3471697202095618\n",
            "Epoch: 18/25\t\tStep: 1/50\t\tloss: 1.3665306568145752\n",
            "Epoch: 18/25\t\tStep: 26/50\t\tloss: 1.3456382568065937\n",
            "Epoch: 19/25\t\tStep: 1/50\t\tloss: 1.4047472476959229\n",
            "Epoch: 19/25\t\tStep: 26/50\t\tloss: 1.3478452700835009\n",
            "Epoch: 20/25\t\tStep: 1/50\t\tloss: 1.3447810411453247\n",
            "Epoch: 20/25\t\tStep: 26/50\t\tloss: 1.3478739857673645\n",
            "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Epoch: 21/25\t\tStep: 1/50\t\tloss: 1.2505446672439575\n",
            "Epoch: 21/25\t\tStep: 26/50\t\tloss: 1.3414726761671214\n",
            "Epoch: 22/25\t\tStep: 1/50\t\tloss: 1.3347729444503784\n",
            "Epoch: 22/25\t\tStep: 26/50\t\tloss: 1.3449087693141057\n",
            "Epoch: 23/25\t\tStep: 1/50\t\tloss: 1.342670202255249\n",
            "Epoch: 23/25\t\tStep: 26/50\t\tloss: 1.3518358239760766\n",
            "Epoch: 24/25\t\tStep: 1/50\t\tloss: 1.3505195379257202\n",
            "Epoch: 24/25\t\tStep: 26/50\t\tloss: 1.3338251297290509\n",
            "Epoch: 25/25\t\tStep: 1/50\t\tloss: 1.3730114698410034\n",
            "Epoch: 25/25\t\tStep: 26/50\t\tloss: 1.3416833464915936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnKv5I2-AmKq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "ee01ed3f-cba2-4e51-db8d-852a76bc52b0"
      },
      "source": [
        "# Model Validation\n",
        "with torch.no_grad():\n",
        "\n",
        "  n_correct, n_samples = 0, 0\n",
        "\n",
        "  for data, labels in test_loader:\n",
        "    \n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    \n",
        "    outputs = model(data)\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    n_samples += labels.shape[0]\n",
        "    n_correct += (predictions == labels).sum().item()\n",
        "\n",
        "  acc = 100 * n_correct/n_samples\n",
        "  print(f\"Acc: {acc}\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e61c4a1c1bec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Model Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mn_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyHX0RYHqFnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO:\n",
        "# Apply normalization to image dataset\n",
        "# Apply augmentation to image dataset\n",
        "# Tensorboard \n",
        "# Improve validation code\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}